{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append('../')\n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PredictとControlの違い\n",
    "\n",
    "状態価値関数$V$のみを求めるものをPredictという\n",
    "\n",
    "状態価値関数$V$と政策$\\pi$を求めるものをControlという"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem: evaluate a given policy pi\n",
    "# Solution: iterative application of Bellman expectation backup\n",
    "# v1 \\to v2 \\to \\cdots \\to v\\pi\n",
    "# Using synchronous backups,\n",
    "#   At each iteration k + 1\n",
    "#   For all state s \\in S\n",
    "#   Update v_{k+1}(s) from v_k(s')\n",
    "#   where s' is a successor state of s\n",
    "\n",
    "# action = env.action_space.sample()\n",
    "# observation, reward, done, info = env.step(action)\n",
    "\n",
    "def policy_eval(policy, env, gamma=1., eps=1e-3):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        Vk = V.copy()\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            for a in range(env.nA):\n",
    "                # P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "                for prob, next_s, reward, done in env.P[s][a]:\n",
    "                    # v += policy[a] * (reward + gamma  * prob * V[next_s])\n",
    "                    # \\pi(a|s)は, sありきのaの確率だから気をつけて\n",
    "                    v += policy[s, a] * (reward + gamma  * prob * V[next_s])\n",
    "            V[s] = v\n",
    "        if np.abs(sum(V - Vk)) < eps:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         -13.99942284 -19.99917026 -21.99908672]\n",
      " [-13.99942284 -17.99929175 -19.99923101 -19.9992398 ]\n",
      " [-19.99917026 -19.99923101 -17.99935111 -13.99951553]\n",
      " [-21.99908672 -19.9992398  -13.99951553   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "V = policy_eval(random_policy, env)\n",
    "print(V.reshape((4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test: Make sure the evaluated policy is what we expected\n",
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "np.testing.assert_array_almost_equal(V, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]\n",
      " [ 0.25  0.25  0.25  0.25]]\n"
     ]
    }
   ],
   "source": [
    "print(np.ones([env.nS, env.nA]) / env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve Policy\n",
    "\n",
    "- Evalue the policy $\\pi$\n",
    "$$v_{\\pi}(s) = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\cdots | S_{t} = s]$$\n",
    "\n",
    "- Improve the policy by acting greedily with respect to $v_{\\pi}$\n",
    "$$\\pi^{'} = \\text{greedy}(v_{\\pi})$$\n",
    "\n",
    "- in Small gridworld improved policy was optimal, $\\pi^{'} = \\pi^{*}$\n",
    "- In general, need more iterations of improvement / evaluation\n",
    "\n",
    "### Policy Improvement\n",
    "\n",
    "- Consider a deterministic policy, $a = \\pi(s)$\n",
    "- We can improve the policy by acting greedily\n",
    "$$\\pi^{'}(s) = arg \\max_{a \\in A}q_{\\pi}(s, a)$$\n",
    "\n",
    "- This improves the value from any state $s$ over one step,\n",
    "$$q_{\\pi}(s, \\pi^{'}(s)) = \\max_{a \\in A}q_{\\pi}(s, a) \\geq q_{\\pi}(s, \\pi(s)) = v_{\\pi}(s)$$\n",
    "\n",
    "- Stop\n",
    "$$v_{\\pi}(s) = \\max_{a \\in A} q_{\\pi}(s, a)$$\n",
    "\n",
    "アイデアとしては, `action_values[a]`という`Q(s,a)`をもつ.    \n",
    "そうすることによって, `choice_a = np.argmax(policy[s])`という選ばれた`s`で最も良い`a`とくらべて,    \n",
    "計算された $$\\text{action_values[a]} = \\sum_{a \\in \\mathcal{A}}  [R + \\gamma \\sum_{s \\in \\mathcal{S}}V[s']]$$\n",
    "が必要になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "def policy_iteration(policy, env, gamma=1., eps=1e-3):\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    while True:\n",
    "        V = policy_eval(policy, env, gamma, eps)\n",
    "        \n",
    "        # Will be set to false if we make any change to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            # The best action we would take under the currect policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "\n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                for prob, next_s, reward, done in env.P[s][a]:\n",
    "                    # これが何かイマイチわかってない\n",
    "                    action_values[a] += reward + gamma * prob * V[next_s]\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            # 最も良いものでなければ終了しない.\n",
    "            # 最も良いものを更新している.\n",
    "            if best_a != chosen_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V, policy = policy_iteration(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 2],\n",
       "       [0, 0, 0, 2],\n",
       "       [0, 0, 1, 2],\n",
       "       [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(np.argmax(policy, axis=1), env.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "- 問題: 最適政策$\\pi$を見つける\n",
    "- 解決策: Bellman最適バックアップの応用反復\n",
    "- $v_1 \\to v_2 \\cdots \\to v_{*}$\n",
    "- 同期バックアップを使う\n",
    "    - $k+1$まで反復する\n",
    "    - すべての状態$s \\in \\mathcal{S}$\n",
    "    - $v_{k}(s')$から$v_{k+1}(s)$を更新する\n",
    "    \n",
    "$$v_{k+1}(s) = \\max_{a \\in \\mathcal{A}} \\bigl( \\mathcal{R_{s}^{a}} + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^{a} v_{k}(s') \\bigr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A = one_step_lookahead(s, V)`が\n",
    "$$\\pi^{'}(s) = arg \\max_{a \\in A}q_{\\pi}(s, a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.):\n",
    "    \"\"\"\n",
    "    Value Iteration Algo.\n",
    "    \n",
    "    Return:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(s, V):\n",
    "        \"\"\"\n",
    "        Helper function to calc the value for all action in a given state.\n",
    "        \n",
    "        \"\"\"\n",
    "        action_vals = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_s, reward, done in env.P[s][a]:\n",
    "                action_vals[a] += reward + discount_factor *  (prob * V[next_s])\n",
    "        return action_vals\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        bef_V = V.copy()\n",
    "        for s in range(env.nS):\n",
    "            v = one_step_lookahead(s, V)  \n",
    "            \"\"\"          \n",
    "            v = []\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                v0 = 0\n",
    "                for prob, next_s, reward, done in env.P[s][a]:\n",
    "                    v0 += reward + discount_factor * (prob * V[next_s])\n",
    "                v.append(v0)\n",
    "            \"\"\"\n",
    "            delta = max(delta, np.abs(max(v) - V[s]))\n",
    "            V[s] = np.max(np.asarray(v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    for s in range(env.nS):\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        policy[s] = np.eye(env.nA)[best_action]\n",
    "            \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy, V = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(V.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 2],\n",
       "       [0, 0, 0, 2],\n",
       "       [0, 0, 1, 2],\n",
       "       [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(np.argmax(policy, axis=1), env.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
